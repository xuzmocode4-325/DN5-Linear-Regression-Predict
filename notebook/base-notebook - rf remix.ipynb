{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c7e849a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T09:24:53.643384Z",
     "start_time": "2021-06-11T09:24:53.622385Z"
    }
   },
   "source": [
    "# Regression Predict Student Solution\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "### Honour Code\n",
    "\n",
    "I {**YOUR NAME, YOUR SURNAME**}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "### Predict Overview: Spain Electricity Shortfall Challenge\n",
    "\n",
    "The government of Spain is considering an expansion of it's renewable energy resource infrastructure investments. As such, they require information on the trends and patterns of the countries renewable sources and fossil fuel energy generation. Your company has been awarded the contract to:\n",
    "\n",
    "- 1. analyse the supplied data;\n",
    "- 2. identify potential errors in the data and clean the existing data set;\n",
    "- 3. determine if additional features can be added to enrich the data set;\n",
    "- 4. build a model that is capable of forecasting the three hourly demand shortfalls;\n",
    "- 5. evaluate the accuracy of the best machine learning model;\n",
    "- 6. determine what features were most important in the model’s prediction decision, and\n",
    "- 7. explain the inner working of the model to a non-technical audience.\n",
    "\n",
    "Formally the problem statement was given to you, the senior data scientist, by your manager via email reads as follow:\n",
    "\n",
    "> In this project you are tasked to model the shortfall between the energy generated by means of fossil fuels and various renewable sources - for the country of Spain. The daily shortfall, which will be referred to as the target variable, will be modelled as a function of various city-specific weather features such as `pressure`, `wind speed`, `humidity`, etc. As with all data science projects, the provided features are rarely adequate predictors of the target variable. As such, you are required to perform feature engineering to ensure that you will be able to accurately model Spain's three hourly shortfalls.\n",
    " \n",
    "On top of this, she has provided you with a starter notebook containing vague explanations of what the main outcomes are. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05600c92",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "742e5d0b",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1.Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Cleaning Data</a>\n",
    "\n",
    "<a href=#four>4. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#five>5. Data Engineering</a>\n",
    "\n",
    "<a href=#six>6. Modeling</a>\n",
    "\n",
    "<a href=#seven>7. Model Performance</a>\n",
    "\n",
    "<a href=#eight>8. Model Explanations</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "997462e2",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "475dbe93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T10:30:53.800892Z",
     "start_time": "2021-06-23T10:30:50.215449Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Libraries for data preparation and model building\n",
    "import scipy as sp\n",
    "import statsmodels as sm\n",
    "import sklearn.model_selection as skl\n",
    "\n",
    "\n",
    "# Setting global constants to ensure notebook results are reproducible\n",
    "PARAMETER_CONSTANT = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f22a6718",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb6c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:35.311495Z",
     "start_time": "2021-06-28T08:49:35.295494Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_train.csv')\n",
    "df_test = pd.read_csv('df_test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81132ab3",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Data Cleaning\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce34c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81eede17",
   "metadata": {},
   "source": [
    "##### The column `Unnamed: 0` is dropped becaues it is an exact replica of the index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c32976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"Unnamed: 0\", axis =1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e4b8a96",
   "metadata": {},
   "source": [
    "##### The dataset has `49 features` and `8763 observations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15376fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eef9ece",
   "metadata": {},
   "source": [
    "##### There are `2068 null entries` in total in the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef81bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks for total nulls in the dataframe\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8274a383",
   "metadata": {},
   "source": [
    "##### The function `check_nulls` tallies the number of nulls per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Checks for total per column\n",
    "def check_nulls (data):\n",
    "    null_dict = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "\n",
    "        if df[col].size > df[col].count():\n",
    "            nulls = df[col].size - df[col].count()\n",
    "            percent_missing = round(nulls / df[col].size * 100)\n",
    "            null_dict[col] = f'has {nulls} nulls making up {percent_missing}% missing values'\n",
    "    return null_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b58886f5",
   "metadata": {},
   "source": [
    "##### Running `check_nulls` exposes that all null values are in one colunm. Namely, `valencia_pressure`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4aa2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_nulls(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4dc68740",
   "metadata": {},
   "source": [
    "##### The column names are all cast to lower case for ease of referencing in the later phases.\n",
    "##### The columns are set in alphabetical order so that the features are grouped by city.\n",
    "##### The time feature is kept in the first index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Order the columns in alphabetical order\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n",
    "\n",
    "# 2. Keep the \"time\" column in the first index position\n",
    "df = df[['time'] + [col for col in df.columns if col != 'time']]\n",
    "\n",
    "# 3. Keep the \"load_shortfall_3h\" column last\n",
    "df = df[[col for col in df.columns if col != 'load_shortfall_3h'] + ['load_shortfall_3h']]\n",
    "\n",
    "# 4. Convert all column titles to lowercase\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f223edbe",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90308f00",
   "metadata": {},
   "source": [
    "##### The `load_shortfall_3h` column name is stored in a `target_variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'load_shortfall_3h'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f1add5b",
   "metadata": {},
   "source": [
    "##### The function `get_categorical_cols` outputs a list of all the columns that are of `object` datatype.\n",
    "##### Other than `time`, there are two others. `seville_pressue` and `valencia_wind_deg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs dataframe of catagorical data\n",
    "def get_categorical_cols(df):\n",
    "    non_numeric = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "          non_numeric.append(col)\n",
    "    return df[non_numeric]\n",
    "\n",
    "categorical_df = get_categorical_cols(df)\n",
    "categorical_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab1fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de481960",
   "metadata": {},
   "source": [
    "##### The function `get_numeric_cols` outputs a list of all the columns that are of `float` or `int` datatype values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdceff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_cols(df):\n",
    "    non_numeric = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != object:\n",
    "          non_numeric.append(col)\n",
    "    return df[non_numeric]\n",
    "\n",
    "numeric_df = get_numeric_cols(df)\n",
    "numeric_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5996b8de",
   "metadata": {},
   "source": [
    "##### The kurtosis of all columns is checked to figure out which columns possibly contain outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287374a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kurtosis_values = df.kurtosis(numeric_only=True)\n",
    "outlier_columns = kurtosis_values[kurtosis_values > 3].index\n",
    "outlier_columns\n",
    "\n",
    "# columns with potential outliers \n",
    "# to be normalisation later on"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d683e76e",
   "metadata": {},
   "source": [
    "##### Using a for loop a list containing the names of the cites is extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187a6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of unique city names \n",
    "unique_cities = []\n",
    "for col in df.drop(['time', 'load_shortfall_3h'], axis=1).columns:\n",
    "    city = col.split('_')[0]\n",
    "    if city not in unique_cities:\n",
    "        unique_cities.append(city)\n",
    "\n",
    "unique_cities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbf9fbba",
   "metadata": {},
   "source": [
    "##### Using `city_df` a unique dataset for all cities is created. This step is mostly for convenience so that more manageable correlation heatmaps can be created later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d15e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dataframe with weather data for each ciy\n",
    "def city_df (df, city_name):\n",
    "\n",
    "    cities = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        city = col.split('_')[0]\n",
    "\n",
    "        if city not in cities:\n",
    "            cities[city] = []\n",
    "\n",
    "            for col in df.columns:\n",
    "                if city in col:\n",
    "                    cities[city].append(col)\n",
    "    \n",
    "    cities[city_name].append(target_variable)\n",
    "    \n",
    "    return df[cities[city_name]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcb37b0e",
   "metadata": {},
   "source": [
    "### Number Summaries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "766dd925",
   "metadata": {},
   "source": [
    "##### The number summaries for the numeric columns reveal an interesting characteristic about all the features related to rain. The columns are mostly filled with `0` entries and have very low varience in their values. These columns are primary candidates for removal during Data Enginnering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edcad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# description of the barcelona weather values\n",
    "barcelona_df = city_df(df, unique_cities[0])\n",
    "barcelona_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb93abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# description of the barcelona weather values\n",
    "bilbao_df = city_df(df, unique_cities[1])\n",
    "bilbao_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d236de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# description of the madrid weather values\n",
    "madrid_df = city_df(df, unique_cities[2])\n",
    "madrid_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a3ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# description of the seville weather values\n",
    "seville_df = city_df(df, unique_cities[3])\n",
    "seville_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5928bc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# description of the seville weather values\n",
    "valencia_df = city_df(df, unique_cities[4])\n",
    "valencia_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef1ef1b8",
   "metadata": {},
   "source": [
    "##### A scatterplot of all the numeric features were plotted against the target feature `load_shortfall`. There were no obvious trends discernible and no feature stands out as either strongly or negatively correlated to the target.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb74182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot relevant feature interactions\n",
    "index = 0\n",
    "fig, axs = plt.subplots(15, 3, figsize=(15, 30))\n",
    "for i in range(15):\n",
    "    for j in range(3):\n",
    "        var = numeric_df.columns[index]\n",
    "        sns.scatterplot(data=numeric_df, x=var, y=target_variable, ax=axs[i,j])\n",
    "        axs[i,j].set_title(f'{var} vs. {target_variable}')\n",
    "        index += 1    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f66a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa739427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at feature distributions\n",
    "plt.figure(figsize =(32,9))\n",
    "sns.histplot(df['load_shortfall_3h'], kde=True)\n",
    "plt.xlabel('Load Shortfall')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of load_shortfall_3h')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "547d73f6",
   "metadata": {},
   "source": [
    "##### We used boxplot visuals to check for possible trends in the non-catagorical data. Nothing significant pops up for any one catagory. Their median values are rougly equivalent and so are their ranges. Although a there is significant enough visible variation in the interquatile ranges for the shortfall amount that this feature can be retained for inclusion in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2405bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in categorical_df.drop(\"time\", axis=1).columns:\n",
    "     plt.figure(figsize =(20,5))\n",
    "     sns.boxplot(x=var, y=target_variable, data = df)\n",
    "     plt.title(f'{var} vs. {target_variable}')\n",
    "     plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49ed1a7a",
   "metadata": {},
   "source": [
    "##### To further explore relationships between features, we plotted correlation heatmaps for each city separately. We observed that although all the features fall within the rrange of weak to no correlation at all, temperature features overall seem to have the strongest relationship with load shortfall, followed by wind degree, which not surprisingly, has a negative correlation (althought still very weak) with the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f821bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables for correlation matrix\n",
    "barca_corr_matrix = barcelona_df.corr(numeric_only=True)\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize =(10,10))\n",
    "sns.heatmap(barca_corr_matrix, annot=True, cmap='coolwarm', xticklabels=True, yticklabels=True)\n",
    "plt.title('Barcelona Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d040803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables for correlation matrix\n",
    "sevi_corr_matrix = seville_df.corr(numeric_only=True)\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize =(10,10))\n",
    "sns.heatmap(sevi_corr_matrix, annot=True, cmap='coolwarm', xticklabels=True, yticklabels=True)\n",
    "plt.title('Seville Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cebd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables for correlation matrix\n",
    "bilb_corr_matrix = bilbao_df.corr(numeric_only=True)\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize =(10,10))\n",
    "sns.heatmap(bilb_corr_matrix, annot=True, cmap='coolwarm', xticklabels=True, yticklabels=True)\n",
    "plt.title('Bilboa Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f3f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables for correlation matrix\n",
    "madr_corr_matrix = madrid_df.corr(numeric_only=True)\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize =(10,10))\n",
    "sns.heatmap(madr_corr_matrix, annot=True, cmap='coolwarm', xticklabels=True, yticklabels=True)\n",
    "plt.title('Madrid Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54939878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables for correlation matrix\n",
    "vale_corr_matrix = valencia_df.corr(numeric_only=True) \n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize =(10,10))\n",
    "sns.heatmap(vale_corr_matrix, annot=True, cmap='coolwarm', xticklabels=True, yticklabels=True)\n",
    "plt.title('Valencia Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4da31864",
   "metadata": {},
   "source": [
    "##### Lastly we took a look at how the shortfall changes over time. It revealed and interesting pattern that looks like it has fluctuations that are seasonal as well as nested cycles that occured at a higher frequency than the broader overall cycle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(32,9))\n",
    "plt.plot(np.arange(len(df[target_variable])), df[target_variable])\n",
    "plt.title(\"load shortfall over time\")\n",
    "plt.xlabel(\"Time (3h)\")\n",
    "plt.ylabel(\"Shortfall\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80e137b6",
   "metadata": {},
   "source": [
    "##### To get clearer analysis on possible time related patterns in shortfall we created frequency tables to aggregate averages for daily and annual cycles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_freqs = {}\n",
    "for index, row in df.iterrows():\n",
    "    hour = row[\"time\"][11:]\n",
    "    if hour in hour_freqs:\n",
    "        hour_freqs[hour][0] += row[target_variable]\n",
    "        hour_freqs[hour][1] += 1\n",
    "    else:\n",
    "        hour_freqs[hour] = [row[target_variable], 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f8e0c48",
   "metadata": {},
   "source": [
    "##### The bar chart shows a very high average shortfall between 12am midday and 3pm and again 12 midnight and 3am for the day cycles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db5073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x-values (hours) and y-values (quotients)\n",
    "hr_x_values = list(hour_freqs.keys())\n",
    "hr_y_values = [value[0] / value[1] for value in hour_freqs.values()]\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(hr_x_values, hr_y_values)\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Avarage Shortfall')\n",
    "plt.title('Average Shortfall Per Three Hour Period')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7a7a16e",
   "metadata": {},
   "source": [
    "##### Below is a frequency table created to check for patterns in the annual cycles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9066ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_freqs = {}\n",
    "for index, row in df.iterrows():\n",
    "    mon = row[\"time\"][5:7]\n",
    "    if mon in mon_freqs:\n",
    "        mon_freqs[mon][0] += row[target_variable]\n",
    "        mon_freqs[mon][1] += 1\n",
    "    else:\n",
    "        mon_freqs[mon] = [row[target_variable], 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ed98b93",
   "metadata": {},
   "source": [
    "##### The bar chart confirms the initial observartion of a cycle within cycles of peaks and reduction in shortfall. There seems to be on average a higher shortfall between July and November and a lower avaerage shortfall between January and May. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec43d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x-values (hours) and y-values (quotients)\n",
    "mn_x_values = list(mon_freqs.keys())\n",
    "mn_y_values = [value[0] / value[1] for value in mon_freqs.values()]\n",
    "\n",
    "mon_list = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"] \n",
    "# Customize the x-labels\n",
    "\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(mn_x_values, mn_y_values)\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Avarage Shortfall')\n",
    "plt.title('Average Shortfall Per Three Hour Period')\n",
    "plt.xticks(ticks=np.arange(12), labels=mon_list, rotation=45, ha='right', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fa93ec6",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 4. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "679d54c0",
   "metadata": {},
   "source": [
    "### Section Goals\n",
    "##### - Create a new dataframe called `df_clean` for engineering purposes.\n",
    "##### - Impute null values for the `valencia_pressue` feature.\n",
    "##### - Remove `rain` and `snow` related columns.\n",
    "##### - Minimize noise by selecting the relevant `temperature` related columns for each city.\n",
    "##### - Convert  the `categorical` columns to `numerical` columns. \n",
    "##### - `Normalize` all the selected features to prepare them for model training. \n",
    "##### - Convert the `time` column to a `datetime` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()\n",
    "df_clean.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49cd758c",
   "metadata": {},
   "source": [
    "##### Imputing `valencia_pressure`\n",
    "##### The median value was selected for the column since it approximates the arithmetic average of the mode and mean values for the feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "valencia_pressure_median = df_clean['valencia_pressure'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc4204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['valencia_pressure'] = df_clean['valencia_pressure'].fillna(valencia_pressure_median, axis=0)\n",
    "df_clean['valencia_pressure'].isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5fcc1b5",
   "metadata": {},
   "source": [
    "##### The next step is transforming the catagorical data into numerical format incase it is kept for training the model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_clean.columns:\n",
    "    if df_clean[col].dtype == object and col != \"time\":\n",
    "        df_clean[col]= df_clean[col].str.extract(r'([0-9]+)')\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cae343b3",
   "metadata": {},
   "source": [
    "##### Removing `rain` and `snow` related columns. \n",
    "##### Since the rain and snow related columns are continuous measurement with little variation in value they are show little significanc as features to train the model on and as such are being dropped from the training data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf1b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = []\n",
    "for col in df_clean.columns:\n",
    "    if \"rain\" in col or \"snow\" in col:\n",
    "        cols_to_remove.append(col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c232c399",
   "metadata": {},
   "source": [
    "##### Temperature columns are highly correlated to each. Since the temp_min column for each city seems to be more correlated to the target in most cases than the other values, this is the one that will be kept for training the model. The rest will be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f047cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_clean.columns:\n",
    "    if \"temp\" in col and \"min\" not in col:\n",
    "        cols_to_remove.append(col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f15ee0f8",
   "metadata": {},
   "source": [
    "##### The rest of the columns are kept removed to preserve homogeny in the train dataset. All five cities have `pressure`, `wind speed`, and `temprature` in common, these seem like th best features to use for training the model since all three factors apply to all of Spain. It is hoped that they could be used to give a good overall picture on how weather impacts energy shortfall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_clean.columns:\n",
    "    if \"clouds\" in col:\n",
    "        cols_to_remove.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b818c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_clean.columns:\n",
    "    if \"weather\" in col:\n",
    "        cols_to_remove.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_clean.columns:\n",
    "    if \"deg\" in col:\n",
    "        cols_to_remove.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad89ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_clean.columns:\n",
    "    if \"humidity\" in col:\n",
    "        cols_to_remove.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a491736",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = []\n",
    "for col in df_clean.columns:\n",
    "    if col not in cols_to_remove:\n",
    "        keep_cols.append(col)\n",
    "\n",
    "keep_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean[keep_cols]\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efd82243",
   "metadata": {},
   "source": [
    "##### In order to prepare the data for our model, converting `seville_pressure` to a numeric feature first needs to take place. Then the data can either be normalized or standardized. This will depend on the presence of outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8071496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0596fcd8",
   "metadata": {},
   "source": [
    "##### Next, the `time` column is converted into datetime format for more accurate time series modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['time'] = pd.to_datetime(df_clean['time'])\n",
    "df_clean['time'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12e84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['time_delta_hours'] = (df_clean['time'] - df_clean['time'].min()).dt.components['hours']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa4cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop(\"time\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f81296",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean[['time_delta_hours'] + [col for col in df_clean.columns if col != 'time_delta_hours']]\n",
    "df_clean.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c68f941c",
   "metadata": {},
   "source": [
    "##### Next, the features are scaled to prepare them for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce2f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating the target varable from the feature variables \n",
    "y = df_clean[target_variable]\n",
    "X = df_clean.drop(target_variable, axis=1)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "# Fit the scaler to the data\n",
    "scaler.fit(X)\n",
    "# Transform the data\n",
    "scaled_data = scaler.transform(X)\n",
    "# Update the dataframe with the scaled data\n",
    "df_clean[X.columns] = scaled_data\n",
    "# Display the scaled data\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59692724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineer existing features for test dataset\n",
    "df_clean_test = df_test\n",
    "# 1. Order the columns in alphabetical order\n",
    "df_clean_test = df_clean_test.reindex(sorted(df_clean_test.columns), axis=1)\n",
    "# 2. Keep the \"time\" column in the first index position\n",
    "df_clean_test = df_clean_test[['time'] + [col for col in df_clean_test.columns if col != 'time']]\n",
    "# 3. Convert all column titles to lowercase\n",
    "df_clean_test.columns = df_clean_test.columns.str.lower()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "180809c3",
   "metadata": {},
   "source": [
    "##### Lastly, the test dataset is put through the same preprocessing as the train dataset has been."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e2d753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_test['valencia_pressure'] = df_clean_test['valencia_pressure'].fillna(valencia_pressure_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_clean_test.columns:\n",
    "    if df_clean_test[col].dtype == object and col != \"time\":\n",
    "        df_clean_test[col]= df_clean_test[col].str.extract(r'([0-9]+)')\n",
    "        df_clean_test[col] = pd.to_numeric(df_clean_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols_test = [col for col in keep_cols if col != target_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_test = df_clean_test[keep_cols_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46701226",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_test['time'] = pd.to_datetime(df_clean_test['time'])\n",
    "df_clean_test['time'].dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53e2ca3b",
   "metadata": {},
   "source": [
    "##### The code snippet `df['time_delta_hours'] = (df['time'] - df['time'].min()).dt.components['hours']` calculates the time difference in hours between each timestamp in the 'TIMESTAMP' column of the DataFrame df and the minimum timestamp value in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e73534",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_test['time_delta_hours'] = (df_clean_test['time'] - df_clean_test['time'].min()).dt.components['hours']\n",
    "df_clean_test = df_clean_test.drop(['time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f60dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_test = df_clean_test[['time_delta_hours'] + [col for col in df_clean_test.columns if col != 'time_delta_hours']] \n",
    "df_clean_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3397c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_test = df_clean_test[['time_delta_hours'] + [col for col in df_clean_test if col != 'time_delta_hours']]\n",
    "#df_clean_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f735ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "# Fit the scaler to the data\n",
    "scaler.fit(df_clean_test)\n",
    "# Transform the data\n",
    "scaled_data = scaler.transform(df_clean_test)\n",
    "# Update the dataframe with the scaled data\n",
    "df_clean_test[df_clean_test.columns] = scaled_data\n",
    "# Display the scaled data\n",
    "df_clean_test.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43b2d523",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90bdc5c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d8959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319ff9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "validations = { }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ced3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(max_depth=5, min_samples_split=2)\n",
    "dt.fit(x_train, y_train)\n",
    "y_pred_val_dt = dt.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735754fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15891208",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=500, random_state=42)\n",
    "rf.fit(x_train, y_train.values.ravel())\n",
    "y_pred_val_rf = rf.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e690386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f550da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "y_pred_val_lr = lr.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f6fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f4e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(x_train, y_train)\n",
    "y_pred_val_rd = ridge.predict(x_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b530251",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a019e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "def rmse(y_val, y_predict):\n",
    "    return np.sqrt(mean_squared_error(y_val, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f14b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the RMSE on the validation set\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred_val_lr))\n",
    "print(\"Validation Set RMSE Linear regression(LR):\", rmse)\n",
    "\n",
    "# Calculate the RMSE on the validation set\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred_val_rd))\n",
    "print(\"Validation Set RMSE Linear regression (RD):\", rmse)\n",
    "\n",
    "# Calculate the RMSE on the validation set\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred_val_rf))\n",
    "print(\"Validation Set RMSE Linear regression(RF):\", rmse)\n",
    "\n",
    "# Calculate the RMSE on the validation set\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred_val_dt))\n",
    "print(\"Validation Set RMSE Linear regression(DT):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best model and motivate why it is the best choice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8ad0c0d",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Explanations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff741c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discuss chosen methods logic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
